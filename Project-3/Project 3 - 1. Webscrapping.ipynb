{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Subreddit Classification: Writing & Blogging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![cover](images/cover.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Blogging and Writing subreddit groups are very similar in nature. Both are communities that are focused on writing. \n",
    "\n",
    "- However, the size of the communities differ, with 67.7K users in the Blogging subreddit, whereas Writing subreddit has 1.4M users. \n",
    "- Based on the following image, we can conclude that the Writing subreddit is far more active than Blogging subreddit, with 1.1k users online at the point of snapshot, and only 96 online for blogging.\n",
    "- Lastly, both of these groups were created at around the same period of time, in Q1 2008.\n",
    "![cover](images/groups_r.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### What is blogging? \n",
    "A blog (a truncation of \"weblog\") is a discussion or informational website published on the World Wide Web (www) consisting of discrete, often informal diary-style text entries (posts).\n",
    "[Source](https://en.wikipedia.org/wiki/Blog)\n",
    "\n",
    "##### What is writing?\n",
    "Writing is a medium of human communication that involves the representation of a language with symbols. Writing systems are not themselves human languages (with the debatable exception of computer languages); they are means of rendering a language into a form that can be reconstructed by other humans separated by time and/or space.\n",
    "[Source](https://en.wikipedia.org/wiki/writing)\n",
    "\n",
    "***In other words, a blogger is also a writer, who writes in the internet through weblogs ('blogs'). However, writing is an art itself, which emphasise the communication through languages. A writer could write anywhere (newspapers, books, magazines, emails, blogs etc.).*** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Business Case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a data scientist in Alytics under the Internet team, my team works with a online influencers and youtubers to improve their views for their posts. \n",
    "\n",
    "A rising trend in the market was for Bloggers to improve their views on their posts through the use of analytics. A subset of this trend is the rise of Reddit as a virtual community for bloggers to ask questions and seek guidance from like-minded individuals. \n",
    "\n",
    "Our team is working on a long-term project targeted to assist bloggers to improve their views on their blog. The first phase of this project would be targeted at the Reddit platform, where bloggers often visit for idea sharing, feedbacks and questions. \n",
    "- **Phase A Part 1**: Create a classifying tool to help bloggers to post their questions and experiences in the correct Subreddit group. **(Current Project!)**\n",
    "- **Phase A Part 2**: We will look into Subreddit analytics to understand how to structure a reddit post to maximise eyeballs (upvotes, comments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Statement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create a text classifier to determine whether a reddit post would be classified into the Subreddit group \"Blogging\" or \"Writing\". \n",
    "\n",
    "We will be measuring the success of our classifier model by looking at several metrics, including accuracy, specificity, sensitivity and model scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Executive Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### EDA\n",
    "From our analysis, the Blogging subreddit group have a huge emphasis on their blog optimisation (common phrases that appear: SEO, traffic flow, keyword search etc.) and less on technical writing elements. However, the Writing subreddit group appears to be the opposite. Posts appear to be focused on writing techniques, with many users posting questions and seeking help for their stories (common phrases that appear: 'writing advice, don know, dont want, help writing, start writing'). \n",
    "\n",
    "In terms of overall tonality of the words/phrases that commonly appear in both subreddit groups, we can conclude Blogging subreddit group appears to be more formal and professional, whilst Writing subreddit group appears to be more casual and community-based. This could be due to the fact that Bloggers are more marketing/promotion oriented, whilst writers are more focues on the art of writing. \n",
    "\n",
    "#### Modelling (Classification Model)\n",
    "<div>\n",
    "<img src=\"images/model.png\" width=\"500\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- As seen above from the models summary, both models have performed similarly in predicting whether posts fall under the Writing or Blogging subreddit groups, with an accuracy of approximately 93%.\n",
    "- From our Logistic regression, we were able to understand how our Logistic regression classify our posts based on the words appeared. \n",
    "<br>\n",
    "<div>\n",
    "<img src=\"images/lr.png\" width=\"700\"/>\n",
    "</div>\n",
    "\n",
    "<div>\n",
    "<img src=\"images/lr2.png\" width=\"700\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Interestingly, words that are of greater importance in classfiying posts into the **Blogging** subreddit are: Posts, Content, Website, Niche, Article, Google, SEO, link etc (web-analytics oriented)\n",
    "- Whereas words that are of greater importance in classfiying posts into the **Writing** subreddit are: story, character book, novel, read, writer, plot, feel, chapter etc. (traditional writing-oriented)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two subreddits that I have scrapped my reddit posts from are:\n",
    "\n",
    "- [Blogging](https://www.reddit.com/r/blogging/)\n",
    "- [Writing](https://www.reddit.com/r/writing/)\n",
    "\n",
    "[**Part 1**](https://github.com/alysesu/GA-Projects/blob/master/Project-3/Project%203%20-%201.%20Webscrapping.ipynb)\n",
    "\n",
    "- Webscrapping\n",
    "\n",
    "[**Part 2**](https://github.com/alysesu/GA-Projects/blob/master/Project-3/Project%203%20-%202.%20Data%20Cleaning%20%26%20EDA.ipynb)\n",
    "- Exploratory Data Analysis\n",
    "- Sentiment Analysis \n",
    "\n",
    "[**Part 3**](https://github.com/alysesu/GA-Projects/blob/master/Project-3/Project%203%20-%203.%20Data%20Processing%20%26%20Modelling.ipynb)\n",
    "- Preprocessing Text Data\n",
    "- Modelling (Logistic Regression)\n",
    "- Modelling (Naive-Bayes Modelling)\n",
    "- Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Webscrapping Imports\n",
    "import requests\n",
    "import time\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "write = 'https://www.reddit.com/r/writing.json'\n",
    "blog = 'https://www.reddit.com/r/Blogging.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_w = requests.get(write,headers={'User-agent': 'GAProj3'})\n",
    "res_b = requests.get(blog,headers={'User-agent': 'GAProj3'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_w.status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_b.status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_dict = res_w.json()\n",
    "blog_dict = res_b.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First Round Scraping (Hot Posts - Default)\n",
    "Accidentally reran the code at the last minute. Hence stopped the process and retained my previous data for EDA and modelling thereafter.\n",
    "Codes below would work. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.reddit.com/r/writing.json\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "posts_w = []\n",
    "after = None\n",
    "\n",
    "for a in range(30):\n",
    "    if after == None:\n",
    "        current_url = write\n",
    "    else:\n",
    "        current_url = write + '?after=' + after\n",
    "    print(current_url)\n",
    "    res_w = requests.get(current_url, headers={'User-agent': 'GAProj3'})\n",
    "    \n",
    "    if res_w.status_code != 200:\n",
    "        print('Status error', res.status_code)\n",
    "        break\n",
    "    \n",
    "    write_dict = res_w.json()\n",
    "    current_posts = [p['data'] for p in write_dict['data']['children']]\n",
    "    posts_w.extend(current_posts)\n",
    "    after = write_dict['data']['after']\n",
    "    \n",
    "    \n",
    "    # generate a random sleep duration to look more 'natural'\n",
    "    sleep_duration = random.randint(2,15)\n",
    "    print(sleep_duration)\n",
    "    time.sleep(sleep_duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "posts_b = []\n",
    "after = None\n",
    "\n",
    "for a in range(30):\n",
    "    if after == None:\n",
    "        current_url = blog\n",
    "    else:\n",
    "        current_url = blog + '?after=' + after\n",
    "    print(current_url)\n",
    "    res_b = requests.get(current_url, headers={'User-agent': 'GAProj3'})\n",
    "    \n",
    "    if res_b.status_code != 200:\n",
    "        print('Status error', res.status_code)\n",
    "        break\n",
    "    blog_dict = res_b.json()    \n",
    "    current_posts = [p['data'] for p in blog_dict['data']['children']]\n",
    "    posts_b.extend(current_posts)\n",
    "    after = blog_dict['data']['after']\n",
    "    \n",
    "    \n",
    "    # generate a random sleep duration to look more 'natural'\n",
    "    sleep_duration = random.randint(2,10)\n",
    "    print(sleep_duration)\n",
    "    time.sleep(sleep_duration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Second round of scraping (New Posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts_w2 = []\n",
    "after = None\n",
    "\n",
    "website_w2 = 'https://www.reddit.com/r/writing/new.json'\n",
    "for a in range(15):\n",
    "    if after == None:\n",
    "        current_url = website_w2\n",
    "    else:\n",
    "        current_url = website_w2 + '?after=' + after\n",
    "    print(current_url)\n",
    "    res_w = requests.get(current_url, headers={'User-agent': 'GAProj3'})\n",
    "    \n",
    "    if res_w.status_code != 200:\n",
    "        print('Status error', res.status_code)\n",
    "        break\n",
    "    \n",
    "    write_dict = res_w.json()\n",
    "    current_posts = [p['data'] for p in write_dict['data']['children']]\n",
    "    posts_w2.extend(current_posts)\n",
    "    after = write_dict['data']['after']\n",
    "    \n",
    "    \n",
    "    # generate a random sleep duration to look more 'natural'\n",
    "    sleep_duration = random.randint(2,15)\n",
    "    print(sleep_duration)\n",
    "    time.sleep(sleep_duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts_b2 = []\n",
    "after = None\n",
    "\n",
    "website_b2 = 'https://www.reddit.com/r/blogging/new.json'\n",
    "for a in range(15):\n",
    "    if after == None:\n",
    "        current_url = website_b2\n",
    "    else:\n",
    "        current_url = website_b2 + '?after=' + after\n",
    "    print(current_url)\n",
    "    res_b = requests.get(current_url, headers={'User-agent': 'GAProj3'})\n",
    "    \n",
    "    if res_b.status_code != 200:\n",
    "        print('Status error', res.status_code)\n",
    "        break\n",
    "    \n",
    "    blog_dict = res_b.json()\n",
    "    current_posts = [p['data'] for p in blog_dict['data']['children']]\n",
    "    posts_b2.extend(current_posts)\n",
    "    after = blog_dict['data']['after']\n",
    "    \n",
    "    # generate a random sleep duration to look more 'natural'\n",
    "    sleep_duration = random.randint(2,15)\n",
    "    print(sleep_duration)\n",
    "    time.sleep(sleep_duration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merging all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Scrape 1 Type (Blogging): {type(posts_b)}')\n",
    "print(f'Scrape 2 Type (Blogging): {type(posts_b2)}')\n",
    "print('\\n')\n",
    "print(f'Scrape 1 Type (Writing): {type(posts_w)}')\n",
    "print(f'Scrape 2 Type (Writing): {type(posts_w2)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'#Posts for Scrape 1 (Blogging): {len(posts_b)}')\n",
    "print(f'#Posts for Scrape 2 (Blogging): {len(posts_b2)}')\n",
    "print('\\n')\n",
    "print(f'#Posts for Scrape 1 (Writing): {len(posts_w)}')\n",
    "print(f'#Posts for Scrape 2 (Writing): {len(posts_w2)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blogposts = posts_b\n",
    "type(blogposts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts_b.extend(posts_b2)\n",
    "len(posts_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writeposts = posts_w\n",
    "type(writeposts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts_w.extend(posts_w2)\n",
    "len(posts_w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Checking\n",
    "### Unique Posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blog= pd.DataFrame(posts_b)\n",
    "write= pd.DataFrame(posts_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blog.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write[['selftext','title']].duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blog[['selftext','title']].duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Initial Observations**\n",
    "- By the looks of our results from our two rounds of webscrape, we see that there are many duplicated posts across both writing and blogging subreddit. This is perhaps due to combining both New and Hot posts. There should be some posts that are duplicated within the 'New' and 'hot' categories as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write.drop_duplicates(subset=['selftext','title'],inplace=True)\n",
    "blog.drop_duplicates(subset=['selftext','title'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(write.shape)\n",
    "print(blog.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exporting Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blog.to_csv('datasets/blogging.csv', index = False)\n",
    "write.to_csv('datasets/writing.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
